{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraping_Professional.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEY4mw8w2wq286I81rcno8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marciocardoso123/DATASCIENCE-/blob/main/Scraping_Professional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91VDvYuBXMVa"
      },
      "outputs": [],
      "source": [
        "#carregando primeira pagina com Scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#abrir aquivo para ler(r)\n",
        "\n",
        "with open('arquivo.html', 'r') as f:\n",
        "\tsoup_string = BeautifulSoup(f.read(), 'html.parser')\n",
        "print(soup_string)\n",
        "\n",
        "#Requisição ao site\n",
        "r = requests.get('http://www.google.com')\n",
        "soup = BeautifulSoup(r.text, 'lxml')\n",
        "print(soup)\n",
        "\n",
        "#abrir arq local\n",
        "with open('arquivo.html', 'r') as f:\n",
        "\tsoup_string = BeautifulSoup(f.read(), 'html5lib')\n",
        "print(soup_string)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Acessar tags html\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "\n",
        "tag = soup.title\n",
        "print(tag)\n",
        "\n",
        "print(tag.name)\n",
        "\n",
        "tag = soup.p\n",
        "print(tag.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fK7UMiK3dPsR",
        "outputId": "c981b729-341c-487d-f36b-cb8d3ce95988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<title>\n",
            "    The Dormouse's story\n",
            "   </title>\n",
            "title\n",
            "p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Open tags especial\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'html.parser')\n",
        "\n",
        "#acessar atrib agrupador class da tag p\n",
        "print(soup.p['class'])\n",
        "\n",
        "#atrib da tag P\n",
        "print(soup.p.attrs)\n",
        "\n",
        "#Recuperar link da tag a com atrib (href)\n",
        "print(soup.a['href'])\n",
        "\n",
        "#acessar o valor do atrib com metodo get\n",
        "print(soup.a.get('href'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbsFAz9yenFK",
        "outputId": "7d3cd47f-f37a-47ec-88ba-762feb270c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['title']\n",
            "{'class': ['title']}\n",
            "http://example.com/elsie\n",
            "http://example.com/elsie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'html5lib')\n",
        "\n",
        "#mostrar o testo com tags\n",
        "#print(soup.prettify())\n",
        "\n",
        "##mostrar o testo de forma legivel\n",
        "#print(soup.get_text())\n",
        "\n",
        "#mostrar o texto da tag p\n",
        "#print(soup.p.get_text())\n",
        "\n",
        "#quando buscamos por texto pegamos tudo na tag mae quando buscamos string precisamos ir até a ultima tag q contem o string \n",
        "#print(soup.p.string)\n",
        "\n",
        "#agora vamos a tag geral e a interior para encontrar a string\n",
        "print(soup.p.b.string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBYaxURsenA8",
        "outputId": "0c38834e-9abd-40a6-a61b-c1f53d610345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "\n",
            "     The Dormouse's story\n",
            "   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Novo arq\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo02.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "\n",
        "print(soup.title)#open title\n",
        "print(soup.head.title)#open head e title\n",
        "print(soup.a)#open tag a\n",
        "print(soup.a.img)#não tem img\n",
        "print(soup.td)#open tela\n",
        "print(soup.tr)#open \n",
        "print(soup.tr.td)#open\n",
        "print(soup.td.attrs)#open o atributo da tag muito importante\n",
        "print(soup.td['class'])#open abri o valor do atributo\n",
        "print(soup.td['class'][0])#open abri o primeiro valor dos atributos que compoe a tag"
      ],
      "metadata": {
        "id": "qCrznRkWdPos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcb89597-c49e-4df3-a0df-19892b524d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<title>Tudo Sobre Google Glass</title>\n",
            "<title>Tudo Sobre Google Glass</title>\n",
            "<a href=\"https://www.facebook.com/TheLifelongTeacher/?fref=ts\" target=\"_blank\">Facebook</a>\n",
            "None\n",
            "<td class=\"ce\">Tela</td>\n",
            "<tr><td class=\"ce\">Tela</td> <td class=\"cd\">Resolução equivalente a tela de 25\"</td></tr>\n",
            "<td class=\"ce\">Tela</td>\n",
            "{'class': ['ce']}\n",
            "['ce']\n",
            "ce\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Navegando pelos filhos das tags\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo02.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "\n",
        "print(soup.table.contents)#todos os filhos da tag table\n",
        "print(type(soup.contents))#tipo dos dados filhos da table\n",
        "print(len(soup.contents))#tamanho da lista\n",
        "\n",
        "print(soup.table.contents[1])#segundo elem da lista\n",
        "print(soup.table.contents[1].span)#acessar o span da tag table\n",
        "print(soup.table.contents[1].span.string)#acesso a conteudo do span\n",
        "print(soup.table.contents[3].td)##acesso a conteudo do terceiro elemento e td\n",
        "\n",
        "#sendo uma lista podemos iterar com for pondo condição \n",
        "#print os filhos de table q são igual a tr\n",
        "for child in soup.table.contents:\n",
        "\tif child.name == 'tr':\n",
        "\t\tprint(child)\n",
        "\n",
        "#se contend é uma lista \n",
        "#children é um iterador de listas\n",
        "print(type(soup.children))\n",
        "\n",
        "#para cada filho in filhos\n",
        "for child in soup.footer.children:\n",
        "\tprint(child)\n",
        "\n",
        "#para cada flho em P onde a tag for 'a' print o atributo\n",
        "for child in soup.footer.p.children:\n",
        "\tif child.name == 'a':\n",
        "\t\tprint(child.get('href'))\n",
        "\n",
        "\n",
        "print(len(list(soup.children)))#print o numero de fihos diretos\n",
        "print(len(list(soup.descendants)))#print o numero de decendentes"
      ],
      "metadata": {
        "id": "2LTeqvOBdUAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#navegando pelos filhos\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import NavigableString, Tag\n",
        "\n",
        "with open('arquivo02.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "\n",
        "print(soup.table.contents)\n",
        "print(type(soup.contents))\n",
        "print(len(soup.contents))\n",
        "\n",
        "print(soup.table.contents[1])\n",
        "print(soup.table.contents[1].span)\n",
        "print(soup.table.contents[1].span.string)\n",
        "print(soup.table.contents[3].td)\n",
        "\n",
        "for child in soup.table.contents:\n",
        "\tif child.name == 'tr':\n",
        "\t\tprint(child)\n",
        "\n",
        "print(type(soup.children))\n",
        "\n",
        "for child in soup.footer.children:\n",
        "\tprint(child)\n",
        "\n",
        "for child in soup.footer.p.children:\n",
        "\tif child.name == 'a':\n",
        "\t\tprint(child.get('href'))\n",
        "\n",
        "print(len(list(soup.children)))\n",
        "print(len(list(soup.descendants)))\n",
        "\n",
        "#para tags em descendants\n",
        "#e se for uma string o filho print a tag\n",
        "#caso n print tag.name\n",
        "for tag in soup.descendants:\n",
        "\tif isinstance(tag, NavigableString):\n",
        "\t\tprint(tag)\n",
        "\telse:\n",
        "\t\tprint(tag.name)\n",
        "  \n",
        "#para tag em decendentes\n",
        "#se for tag print tag \n",
        "for tag in soup.descendants:\n",
        "\tif isinstance(tag, Tag):\n",
        "\t\tprint(tag)\n",
        "\n",
        "# diferença entre strings e stripped_strings\n",
        "#para string em aside\n",
        "#print represente valor que o python reconheça\n",
        "for string in soup.aside.strings:\n",
        "\tprint(repr(string))\n",
        "\n",
        "#para string em aside\n",
        "#print represente valor que o python reconheça\n",
        "#stripped remove espaços em branco\n",
        "for string in soup.aside.stripped_strings:\n",
        "\tprint(repr(string))"
      ],
      "metadata": {
        "id": "Q_55y3xpdT8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# consulta desde a tag pai ou parent\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo02.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "'''\n",
        "print(soup.parent)# a consulta feita na tag html pai n tem resposta pois n tem nada antes dela\n",
        "\n",
        "tag_title = soup.title\n",
        "print(tag_title)# tag title\n",
        "print(tag_title.parent)# consulta o pai da tag title\n",
        "print(soup.td.parent)\n",
        "print(soup.td.parent.parent)#consulta a tag pai do pai\n",
        "'''\n",
        "\n",
        "for pai in soup.p.parents:\n",
        "\tprint(pai.name)"
      ],
      "metadata": {
        "id": "rsRBdtm8kgBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#consulta a irmaos \n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo02.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "\n",
        "#print(soup.next_sibling)#htnl n tem irmaos\n",
        "#print(soup.td)# irmaos de td\n",
        "#print(soup.td.parent)#pai\n",
        "#print(soup.td.next_sibling)#irmao\n",
        "#print(soup.td.next_sibling.next_sibling)#irmao do irmao\n",
        "\n",
        "'''\n",
        "#irmaos posteriores\n",
        "for sibling in soup.p.next_siblings:\n",
        "\tprint(repr(sibling))\n",
        "'''\n",
        "#irmaos anteriores\n",
        "for sibling in soup.p.previous_siblings:\n",
        "\tprint(repr(sibling))"
      ],
      "metadata": {
        "id": "JAqz_v9adPls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#entre elementos\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Tag\n",
        "\n",
        "with open('arquivo03.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "\n",
        "#print(soup.div.next_element.next_element)#elementos apos a div e quanto mais next vai pulando os elementos\n",
        "#print(soup.li.previous_element.previous_element)#elementos antes da div\n",
        "\n",
        "'''\n",
        "#elementos apos\n",
        "for e in soup.ul.next_elements:\n",
        "\tif isinstance(e, Tag):\n",
        "\t\tprint(e)\n",
        "'''\n",
        "#elementos antes\n",
        "for e in soup.li.previous_elements:\n",
        "\tprint(repr(e))"
      ],
      "metadata": {
        "id": "etVupadXdPie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Buscando elementos com find\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo04.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "\n",
        "'''\n",
        "tag = soup.find('li')#consulta por li\n",
        "print(tag)\n",
        "'''\n",
        "tag = soup.find(string='plants')#consulta por string\n",
        "print(tag)\n",
        "\n",
        "tag = soup.find(string='bear')#consulta por string\n",
        "print(tag)\n",
        "\n",
        "tag = soup.find(id='primaryconsumers')#consulta por id\n",
        "print(tag)\n",
        "\n",
        "tag = soup.find(attrs={'class':'primaryconsumerlist'})#consulta por class de css\n",
        "print(tag)\n",
        "\n",
        "tag = soup.find(class_='primaryconsumerlist')#consulta por class \n",
        "print(tag)\n",
        "\n",
        "tag = soup.find('li', attrs={'class':'producerlist'})#consulta por class dentro de li\n",
        "print(tag)\n",
        "\n",
        "tag = soup.ul.li.find('li')#consulta por tag\n",
        "print(tag)\n",
        "'''\n",
        "'''\n",
        "#função para interagir com find q busca apenas a primeira ocorrencia\n",
        "#se ouver dentro de atributos id e dentro de id secundary consumers\n",
        "def is_sencondary_consumers(tag):\n",
        "\treturn tag.has_attr('id') and tag.get('id') == 'secondaryconsumers'\n",
        "\n",
        "secondary_consumer = soup.find(is_sencondary_consumers)\n",
        "print(secondary_consumer.li.div.string)"
      ],
      "metadata": {
        "id": "RmBt6OQ9dPe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find all busca por todas as ocorrencias \n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo03.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "\n",
        "'''\n",
        "tag_list = soup.find_all('ul')#buscar todas occoorencias de ul\n",
        "print(tag_list)\n",
        "\n",
        "tag_list = soup.find_all(['ul', 'div'])#buscar todas occoorencias de ul e div\n",
        "print(tag_list)\n",
        "\n",
        "tag_list = soup.find_all('ul', limit=2)#buscar todas occoorencias de ul até 2 ocorrencias\n",
        "print(tag_list)\n",
        "\n",
        "tag_list = soup.find_all(string='rabbit')\n",
        "print(tag_list)\n",
        "\n",
        "tag_list = soup.find_all(string=True)#buscar todos os textos em ocorrencia de lista\n",
        "print(tag_list)\n",
        "\n",
        "tag_list = soup.find_all(string=['rabbit', 'bear'])\n",
        "print(tag_list)\n",
        "\n",
        "tag_list = soup.find_all(class_=['producerlist', 'primaryconsumerlist'])#buscar todas occoorencias das classes\n",
        "print(tag_list)\n",
        "\n",
        "tag_list = soup.ul.find_all('div')\n",
        "print(tag_list)\n",
        "'''\n",
        "\n",
        "tag_list = soup.find_all('div', class_='name')#buscar todas occoorencias de div e class name\n",
        "print(tag_list)"
      ],
      "metadata": {
        "id": "hAz6rnOEdPbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tipos de consultas\n",
        "#Find e find-all buscam para baixo na arvore\n",
        "#find_parent e find+parents buscam para cima\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo04.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "\n",
        "\n",
        "primaryconsumers = soup.find_all(class_='primaryconsumerlist')\n",
        "primaryconsumer = primaryconsumers[0]\n",
        "#print(primaryconsumer)\n",
        "\n",
        "'''\n",
        "parent_ul = primaryconsumer.find_parents('ul')\n",
        "print(parent_ul)\n",
        "'''\n",
        "\n",
        "parent_ul = primaryconsumer.find_parent('ul')\n",
        "print(parent_ul)"
      ],
      "metadata": {
        "id": "djEjaCrPOukk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wLhbF3y1Ouhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# busca por irmãos\n",
        "#find_next_sibling() << busca irmaos para baixo\n",
        "#find_previous_sibling() << busca irmaos para cima\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo03.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "\n",
        "'''\n",
        "#procurar irmao abaixo de id=producers \n",
        "producers = soup.find(id='producers')\n",
        "next_sibling = producers.find_next_sibling()\n",
        "print(next_sibling)\n",
        "'''\n",
        "\n",
        "'''\n",
        "#procurar irmaos abaixo de id=producers \n",
        "producers = soup.find(id='producers')\n",
        "next_siblings = producers.find_next_siblings()\n",
        "print(next_siblings)\n",
        "'''\n",
        "\n",
        "'''\n",
        "#procurar irmao acima de id=producers \n",
        "producers = soup.find(id='quaternaryconsumers')\n",
        "previous_sibling = producers.find_previous_sibling()\n",
        "print(previous_sibling)\n",
        "'''\n",
        "#procurar irmaos acima de id=producers\n",
        "producers = soup.find(id='quaternaryconsumers')\n",
        "previous_siblings = producers.find_previous_siblings()\n",
        "print(previous_siblings)"
      ],
      "metadata": {
        "id": "AkYuo7OXnGVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#buscar pelo proximo e anterior elemento\n",
        "#find_next << proximo para baixo\n",
        "#find_next_all << proximos para baixo\n",
        "#find_previous << anterior up\n",
        "#finf_all_previous << anteriores up\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('arquivo03.html', 'r') as f:\n",
        "\tsoup = BeautifulSoup(f, 'lxml')\n",
        "\n",
        "'''\n",
        "producers = soup.find(id='producers')\n",
        "tag_next = producers.find_next()\n",
        "print(tag_next)\n",
        "\n",
        "producers = soup.find(id='producers')\n",
        "tag_next = producers.find_all_next()\n",
        "print(tag_next)\n",
        "\n",
        "producers = soup.find(id='quaternaryconsumers')\n",
        "tag_previous = producers.find_previous()\n",
        "print(tag_previous)\n",
        "'''\n",
        "\n",
        "producers = soup.find(id='quaternaryconsumers')\n",
        "tag_previous = producers.find_all_previous()\n",
        "print(tag_previous)"
      ],
      "metadata": {
        "id": "mIqh6HMRnGST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metodo get \n",
        "#numa consulta no youtube sobre strogonof \n",
        "#dividir endereço até '?'\n",
        "#payload = consulta sem + entre palavras\n",
        "import requests\n",
        "\n",
        "url = 'https://www.youtube.com/results?'\n",
        "\n",
        "payload = {'search_query':'como fazer estrogonofe de frango'}\n",
        "\n",
        "r = requests.get(url, params=payload)\n",
        "\n",
        "print(r.text)\t\n",
        "print(r.url)\n",
        "print(r.encoding)\n",
        "\n",
        "with open('youtube.html', 'wb') as f:\n",
        "\tf.write(r.content)"
      ],
      "metadata": {
        "id": "IBl0rMVCnGO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metodo post \n",
        "#\n",
        "#\n",
        "import requests\n",
        "\n",
        "#Método post\n",
        "url = 'http://www.buscacep.correios.com.br/sistemas/buscacep/resultadoBuscaCepEndereco.cfm'\n",
        "#importante criar dicionario para passagem de parametros {chave valor}\n",
        "payload = {'relaxation':'12240000',\n",
        "\t\t\t'tipoCEP':'ALL',\n",
        "\t\t\t'semelhante':'N'}\n",
        "#response = requisição que pede os valores q correspondem a payload\n",
        "response = requests.post(url, data=payload)\n",
        "\n",
        "with open('correios.html', 'w') as f:\n",
        "\tf.write(response.text)\n"
      ],
      "metadata": {
        "id": "7OTcXUfrF8Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#requisição do status code do site\n",
        "#\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = 'http://www.google.com/'\n",
        "\n",
        "r = requests.get(url)\n",
        "\n",
        "print(r.status_code)\n",
        "#exemplo para montar programa começar por conferir o status do site alvo\n",
        "if r.status_code == requests.codes.ok:\n",
        "\tsoup = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "print(r.request.headers)"
      ],
      "metadata": {
        "id": "K9QswfKoF74V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simulação de rebebimento de cabeçalho http\n",
        "#\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'http://michaelis.uol.com.br/busca?'\n",
        "\n",
        "payload = {'r':'0',\n",
        "\t\t\t'f':'0',\n",
        "\t\t\t't':'0',\n",
        "\t\t\t'palavra':'talk'}\n",
        "#request o site/usuario/html/linguagem/encoding/tendo como referencia o endereço ...\n",
        "header = {'(Request-Line)':'GET /busca?r=0&f=0&t=0&palavra=talk HTTP/1.1',\n",
        "\t\t'Host':\t'michaelis.uol.com.br',\n",
        "\t\t'User-Agent':'Mozilla/5.0 (Windows NT 6.3; WOW64; rv:47.0) Gecko/20100101 Firefox/47.0',\n",
        "\t\t'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "\t\t'Accept-Language':'pt-BR,pt;q=0.8,en-US;q=0.5,en;q=0.3',\n",
        "\t\t'Accept-Encoding':'gzip, deflate',\n",
        "\t\t'Referer':'http://michaelis.uol.com.br/'}\n",
        "\n",
        "r = requests.get(url, params=payload, headers=header)\n",
        "\n",
        "soup = BeautifulSoup(r.text, 'lxml')\n",
        "\n",
        "div = soup.find('div', {'id':'content'})\n",
        "print(div.get_text())\n",
        "\n",
        "with open('michaelis.html', 'w', encoding='utf-8') as f:\n",
        "\tf.write(r.text)\n",
        "\n",
        "print(r.request.headers)"
      ],
      "metadata": {
        "id": "Svn2mqYNOueO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cookies é um aquivo q o site deixa no navegador \n",
        "#e qnd o usurario retorna ele carrega histórico\n",
        "import requests\n",
        "\n",
        "url = 'http://www.submarino.com.br/'\n",
        "\n",
        "\n",
        "r = requests.get(url)\n",
        "cookie = r.cookies.get_dict()\n",
        "\n",
        "busca = 'notebook'\n",
        "url = 'http://busca.submarino.com.br/busca.php?q={0}'.format(busca)\n",
        "\n",
        "r = requests.get(url, cookies=cookie)\n",
        "with open('submarino.html', 'w') as f:\n",
        "\tf.write(r.text)"
      ],
      "metadata": {
        "id": "ICYxrZsVNnFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Acompanhar redirecionamento de um site\n",
        "#\n",
        "#Tracking redirection of the request using\n",
        "import requests\n",
        "\n",
        "r = requests.get('http://google.com')\n",
        "\n",
        "#mostra que a url não é mais a mesma, ela foi redirecionada\n",
        "print(r.url)\n",
        "#mostrar o código 302 que é o código de redirecionamento\n",
        "print(r.status_code)\n",
        "#mostrar que não está vazio o historico de direcionamento\n",
        "print(r.history)\n",
        "#mostra o primeiro redirecionamento e conforme for trocando o valor do history mostra outros\n",
        "print(r.history[0].status_code)\n",
        "#mostrar o headers do redirecionamento\n",
        "print(r.history[0].headers)\n",
        "#mostrar a url para qual foi redirecionada\n",
        "print(r.history[0].headers['Location'])\n",
        "\n",
        "#mostrar que o parametro allow_redirects=False, ele evita redirecionamento. o valor True é por default, \n",
        "#mas pode ser colocar True também.\n",
        "r = requests.get('http://google.com', allow_redirects=False)\n",
        "\n",
        "#ai só imprimir para mostrar que é verdade, pois a url atual é a mesma.\n",
        "print(r.url)\n",
        "\n",
        "#mostrar que está vazio\n",
        "print(r.history)\n"
      ],
      "metadata": {
        "id": "kuzJBWM5Nm9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tempo de request\n",
        "#\n",
        "#Aula 11 Usando timeout\n",
        "import requests\n",
        "\n",
        "#mostrar que o timeout é um tempo limite de requests, caso ultrapasse esse tempo de espera\n",
        "#retornara um erro.\n",
        "r = requests.get('http://google.com', timeout=0.03)\n",
        "\n",
        "#If the remote server is very slow, you can tell Requests to wait forever for a response, \n",
        "#by passing None as a timeout value and then retrieving a cup of coffee.\n",
        "r = requests.get('http://google.com', timeout=None)\n",
        "\n",
        "#The timeout value will be applied to both the connect and the read timeouts. \n",
        "#Specify a tuple if you would like to set the values separately:\n",
        "r = requests.get('http://google.com', timeout=(3.05, 27))\n",
        "\n"
      ],
      "metadata": {
        "id": "0MmlraumNm6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tratando erros e execessoes\n",
        "#\n",
        "import requests\n",
        "\n",
        "url = 'http://google.com/'\n",
        "\n",
        "try:\n",
        "    r = requests.get(url, timeout=0.03)\n",
        "\t\n",
        "\t#The request timed out while trying to connect to the remote server.\n",
        "  #connection error é quando o temmpo execede e n tem resposta\n",
        "except requests.exceptions.ConnectionError as e:\n",
        "\tprint('ConnectionError')\n",
        " \n",
        "\t#When there are invalid HTTP responses, Requests will raise an HTTPError exception\n",
        "  #http error é quendo a resposta vem errada\n",
        "except requests.exceptions.HTTPError as e:\n",
        "\tprint ('HTTPError')\n",
        " \n",
        "\t#If the request gets timed out, this exception will be raised\n",
        "  #pedido expirou \n",
        "except requests.exceptions.Timeout as e:\n",
        "\tprint ('Timeout')\n",
        " \n",
        "    # Maybe set up for a retry, or continue in a retry loop\n",
        "    #erro por execesso de rquisições como loop \n",
        "except requests.exceptions.TooManyRedirects as e:\n",
        "\tprint ('TooManyRedirects')\n",
        " \n",
        "    # Tell the user their URL was bad and try a different one\n",
        "    #quando é passada uma url invalida\n",
        "except requests.exceptions.RequestException as e:\n",
        "    # catastrophic error. bail.\n",
        "\tprint ('RequestException')"
      ],
      "metadata": {
        "id": "Ia6NVCw7OubD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#request com arq Json\n",
        "#\n",
        "import requests\n",
        "\n",
        "url = 'http://compras.dados.gov.br'\n",
        "cnpj = '07689002000189' #Embraer\n",
        "\n",
        "url = '{0}/contratos/v1/contratos.json?cnpj_contratada={1}'.format(url, cnpj)\n",
        "\n",
        "r = requests.get(url)\n",
        "print(r.json())\n",
        "print(r.json()['_embedded']['contratos'])\n",
        "print(r.json()['_embedded']['contratos'][0])"
      ],
      "metadata": {
        "id": "S3_oQknaOuXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#session object varias requisições\n",
        "#\n",
        "import requests\n",
        "\n",
        "url = 'http://www.submarino.com.br/'\n",
        "\n",
        "s = requests.Session()\n",
        "s.get(url)\n",
        "\n",
        "busca = 'notebook'\n",
        "url = 'http://busca.submarino.com.br/busca.php?q={0}'.format(busca)\n",
        "\n",
        "r = s.get(url)\n",
        "with open('submarino.html', 'w') as f:\n",
        "\tf.write(r.text)"
      ],
      "metadata": {
        "id": "5NdiTRkmdPTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#proxe é ip contratado para acesso a site que limitem o numero de consultas\n",
        "#\n",
        "import requests\n",
        "\n",
        "#http://www.ultraproxies.com/\n",
        "url = 'https://www.hide-my-ip.com/pt/proxylist.shtml'\n",
        "\n",
        "proxies = {'http':'218.207.102.106:81'}\n",
        "\n",
        "try:\n",
        "\tr = requests.get(url, proxies=proxies)\n",
        "\tprint(r.status_code)\n",
        "except requests.exceptions.ConnectionError as e:\n",
        "\tprint(str(e))"
      ],
      "metadata": {
        "id": "dF8Hp_WTdPNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#como baixar uma iamgem\n",
        "#\n",
        "import requests\n",
        "\n",
        "url = 'http://st2.depositphotos.com/1514397/5893/i/450/depositphotos_58937687-Woman-putting-roller-skates.jpg'\n",
        "\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('img.jpg', 'wb') as f:\n",
        "\tf.write(r.content)"
      ],
      "metadata": {
        "id": "cAVBN7Wm829L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#como acessar um site \n",
        "#como fazer autenticação com request\n",
        "#\n",
        "import requests\n",
        "\n",
        "url = 'http://httpbin.org/basic-auth/user/passwd'\n",
        "\n",
        "r = requests.get(url)\n",
        "print(r.status_code)\n",
        "\n",
        "r = requests.get(url, auth=('user', 'passwd'))\n",
        "print(r.status_code)\n"
      ],
      "metadata": {
        "id": "-wTYDDsV8259"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MdqOx4qC822d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5x0pT1sd82zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rsc_-xdy82vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XMKUlg-edPJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nfYZ4DDHdPFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}